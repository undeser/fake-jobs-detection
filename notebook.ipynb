{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mervseah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17880, 18)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji, string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "df = pd.read_csv('DataSet.csv')\n",
    "# Set pandas option to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17880, 18)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.drop_duplicates(subset=['title','location', 'department' , 'salary_range', 'company_profile', 'description', 'requirements', 'benefits','telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'industry', 'function', 'fraudulent'], keep='first', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation Test Split (Text and Categorical)\n",
    "Train-Test-Validation: 70-15-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12516, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(947, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4417, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df.drop('fraudulent', axis=1, inplace=False)\n",
    "y = df['fraudulent']  # Target variable to predict\n",
    "\n",
    "## splitting into training (70%) and remaining (30%)\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "## splitting remaining into test (15%) and validation (15%)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rem, y_rem, test_size = 0.15 / 0.85, random_state=42)\n",
    "\n",
    "for dataset in [X_train, X_val, X_test]:\n",
    "    display(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation Test Split (No Combined Cleaned Text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12516, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(947, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4417, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_to_drop = ['fraudulent', 'title', 'benefits', 'description', 'requirements', 'company_profile']\n",
    "X_cat = df.drop(columns_to_drop, axis = 1)\n",
    "\n",
    "## splitting into training (70%) and remaining (30%)\n",
    "X_train_cat, X_rem_cat, y_train_cat, y_rem_cat = train_test_split(X_cat, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "## splitting remaining into test (15%) and validation (15%)\n",
    "X_test_cat, X_val_cat, y_test_cat, y_val_cat = train_test_split(X_rem_cat, y_rem_cat, test_size = 0.15 / 0.85, random_state=42)\n",
    "\n",
    "for dataset in [X_train_cat, X_val_cat, X_test_cat]:\n",
    "    display(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEXT PREPROCESSING FOR TEXTUAL COLUMNS\n",
    "\n",
    "def clean_text_features(text):\n",
    "    try:\n",
    "        if pd.isnull(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]*>', '', text)\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove non-alphanumeric characters (except specified punctuation)\n",
    "        text = re.sub(r'[^\\w\\s.,:;!?\\'\"-]', '', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove emojis\n",
    "        text = emoji.replace_emoji(text, replace=\"\")\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        text = \" \".join(filtered_words)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "text_columns = ['title', 'benefits', 'description', 'requirements', 'company_profile']\n",
    "\n",
    "for col in text_columns:\n",
    "    X_train[col] = X_train[col].apply(clean_text_features)\n",
    "    X_val[col] = X_val[col].apply(clean_text_features)\n",
    "    X_test[col] = X_test[col].apply(clean_text_features)\n",
    "\n",
    "X_train['combined_text_data'] = (\n",
    "    X_train['title'] + ' ' +\n",
    "    X_train['description'] + ' ' +\n",
    "    X_train['requirements'] + ' ' +\n",
    "    X_train['company_profile'] + ' ' +\n",
    "    X_train['benefits']\n",
    ")\n",
    "\n",
    "X_val['combined_text_data'] = (\n",
    "    X_val['title'] + ' ' +\n",
    "    X_val['description'] + ' ' +\n",
    "    X_val['requirements'] + ' ' +\n",
    "    X_val['company_profile'] + ' ' +\n",
    "    X_val['benefits']\n",
    ")\n",
    "\n",
    "X_test['combined_text_data'] = (\n",
    "    X_test['title'] + ' ' +\n",
    "    X_test['description'] + ' ' +\n",
    "    X_test['requirements'] + ' ' +\n",
    "    X_test['company_profile'] + ' ' +\n",
    "    X_test['benefits']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encoding categorical columns\n",
    "\n",
    "categorical_columns = ['location', 'department', 'employment_type', 'required_experience', \n",
    "                       'required_education', 'industry', 'function']\n",
    "\n",
    "encoders = {} ## storing this here first if we wanna use them later\n",
    "\n",
    "for column in categorical_columns:\n",
    "    if column in X_train.columns:\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        encoders[column] = le\n",
    "\n",
    "        for dataset in [X_train, X_train_cat]:\n",
    "\n",
    "            dataset[column] = dataset[column].fillna('Unknown') \n",
    "            dataset[f\"{column}_encoded\"] = le.fit_transform(dataset[column])\n",
    "\n",
    "        def transform_with_unknown(encoder, series):\n",
    "            return series.apply(lambda x: encoder.transform([x])[0] if x in encoder.classes_ else -1)\n",
    "\n",
    "        for dataset in [X_val, X_test, X_val_cat, X_test_cat]:\n",
    "\n",
    "            dataset[column] = dataset[column].fillna('Unknown') \n",
    "            dataset[f\"{column}_encoded\"] = transform_with_unknown(le, dataset[column]) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encodign binary columns\n",
    "\n",
    "'''\n",
    "important: idk if we should keep in_balanced_dataset\n",
    "'''\n",
    "binary_columns = ['telecommuting', 'has_company_logo', 'has_questions', 'fraudulent', 'in_balanced_dataset']\n",
    "\n",
    "for column in binary_columns:\n",
    "    if column in X_train.columns:\n",
    "        for dataset in [X_train, X_val, X_test, X_train_cat, X_val_cat, X_test_cat]:\n",
    "            dataset[column] = dataset[column].apply(lambda x: 1 if x == 't' else (0 if x == 'f' else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## numerical columns\n",
    "\n",
    "columns_to_normalize = ['salary_lower', 'salary_upper', 'salary_average', 'salary_range_diff']\n",
    "\n",
    "for dataset in [X_train, X_val, X_test, X_train_cat, X_val_cat, X_test_cat]:\n",
    "    dataset[['salary_lower', 'salary_upper']] = dataset['salary_range'].str.split('-', expand=True).astype(float)\n",
    "    dataset['salary_average'] = (dataset['salary_lower'] + dataset['salary_upper']) / 2\n",
    "    dataset['salary_range_diff'] = dataset['salary_upper'] - dataset['salary_lower']\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    dataset[columns_to_normalize] = scaler.fit_transform(dataset[columns_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING RAW TEXTUAL COLUMNS THAT ARE UNUSED\n",
    "\n",
    "columns_to_drop = ['title', 'benefits', 'description', 'requirements', 'company_profile']\n",
    "for dataset in [X_train, X_val, X_test]:\n",
    "    dataset.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "# df.drop(columns_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings for textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorizer Embedding \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X_train['combined_text_data'])\n",
    "X_train_count =  count_vect.transform(X_train['combined_text_data'])\n",
    "X_val_count = count_vect.transform(X_val['combined_text_data'])\n",
    "X_test_count = count_vect.transform(X_test['combined_text_data'])\n",
    "\n",
    "#TFIDF Embedding \n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train['combined_text_data'])\n",
    "X_train_tfidf = tfidf_vect.transform(X_train['combined_text_data'])\n",
    "X_val_tfidf = tfidf_vect.transform(X_val['combined_text_data'])\n",
    "X_test_tfidf = tfidf_vect.transform(X_test['combined_text_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12516, 60665)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(947, 60665)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4417, 60665)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train_count.shape)\n",
    "display(X_val_count.shape)\n",
    "display(X_test_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12516, 60598)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(947, 60598)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4417, 60598)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train_tfidf.shape)\n",
    "display(X_val_tfidf.shape)\n",
    "display(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = LogisticRegression(max_iter= 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'US, IN, Indianapolis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model to the training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_cat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set (to minimize loss)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m val_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val_cat)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1508\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1506\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1508\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1516\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 964\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[1;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    750\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m-> 2070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'US, IN, Indianapolis'"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "model.fit(X_train_cat, y_train)\n",
    "\n",
    "# Evaluate on validation set (to minimize loss)\n",
    "val_pred_proba = model.predict_proba(X_val_cat)[:, 1]\n",
    "val_auc = roc_auc_score(y_val, val_pred_proba)\n",
    "print(f\"Validation AUC-ROC: {val_auc:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_pred = model.predict(X_test_cat)\n",
    "test_pred_proba = model.predict_proba(X_test_cat)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_pred)\n",
    "precision = precision_score(y_test, test_pred)\n",
    "recall = recall_score(y_test, test_pred)\n",
    "f1 = f1_score(y_test, test_pred)\n",
    "auc = roc_auc_score(y_test, test_pred_proba)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "print(f\"Test AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model to the training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X_train_count, y_train)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set (to minimize loss)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m val_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val_count)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "model.fit(X_train_count, y_train)\n",
    "\n",
    "# Evaluate on validation set (to minimize loss)\n",
    "val_pred_proba = model.predict_proba(X_val_count)[:, 1]\n",
    "val_auc = roc_auc_score(y_val, val_pred_proba)\n",
    "print(f\"Validation AUC-ROC: {val_auc:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_pred = model.predict(X_test_count)\n",
    "test_pred_proba = model.predict_proba(X_test_count)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_pred)\n",
    "precision = precision_score(y_test, test_pred)\n",
    "recall = recall_score(y_test, test_pred)\n",
    "f1 = f1_score(y_test, test_pred)\n",
    "auc = roc_auc_score(y_test, test_pred_proba)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "print(f\"Test AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate on validation set (to minimize loss)\n",
    "val_pred_proba = model.predict_proba(X_val_tfidf)[:, 1]\n",
    "val_auc = roc_auc_score(y_val, val_pred_proba)\n",
    "print(f\"Validation AUC-ROC: {val_auc:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_pred = model.predict(X_test_tfidf)\n",
    "test_pred_proba = model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_pred)\n",
    "precision = precision_score(y_test, test_pred)\n",
    "recall = recall_score(y_test, test_pred)\n",
    "f1 = f1_score(y_test, test_pred)\n",
    "auc = roc_auc_score(y_test, test_pred_proba)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "print(f\"Test AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, test_pred_proba)\n",
    "\n",
    "# Calculate the AUC score\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2, label='Random Guess')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.6 ('project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "643a9d2ca26ce45b27386ef8940c8ce0d888c70cc45f61d60264c2b0e58d842b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
